{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install scikit-learn\n",
        "!pip install wandb"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_Fs0gUqylGJE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dLA2PCSQK5Hl"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "import evaluate\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import wandb\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "dataset = load_dataset(\"emotion\")\n",
        "\n",
        "# Tokenizer and model\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
        "\n",
        "# Tokenize the dataset\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Apply tokenizer to the dataset\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "Fv-qz1gVLJmm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define compute_metrics function\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "597E-Er6JEoG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained model evaluation\n",
        "\n",
        "base_eval_results = trainer.evaluate()\n",
        "print(\"Base Model:\", base_eval_results)"
      ],
      "metadata": {
        "id": "ABj9Klxel97i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "GovgnOm-tiPT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update arguments\n",
        "\n",
        "fine_tune_args = TrainingArguments(\n",
        "    output_dir=\"./peft_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=fine_tune_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "-Or-zuFTt8LD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune\n",
        "\n",
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "FXxTOhWtHIav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "peft_model.save_pretrained(\"./peft_model\")"
      ],
      "metadata": {
        "id": "cdk60e76YmNV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForSequenceClassification\n",
        "from peft import PeftModel\n",
        "\n",
        "\"\"\"\n",
        "I couldn't load the Peft model using 'AutoPeftModelForSequenceClassification' because it encounters a problem\n",
        "with the number of labels (6),it reverts it to the default binary (2)\n",
        "\"\"\"\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, \"./peft_model\")\n",
        "\n",
        "final_trainer = Trainer(\n",
        "    model=fine_tuned_model,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "\n",
        "fine_tuned_eval_results = final_trainer.evaluate()\n",
        "print(\"Fine-Tuned Model:\", fine_tuned_eval_results)"
      ],
      "metadata": {
        "id": "7VaVYnAwYnem"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}